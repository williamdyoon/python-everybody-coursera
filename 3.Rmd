---
output: 
  html_document: 
    keep_md: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, engine='python')
```
#Python For Everybody
##3 - Using Python to Access Web Data

###0) python version used
```{r}
import sys
print(sys.version)
```

###1) regex simple examples
```{r}
import re
hand = open('mbox-short.txt')
for line in hand:
    line = line.rstrip()
    if re.search('^From: ', line):
        print line
        break
x = 'My 2 favorite numbers are 19 and 42'
y = re.findall('[0-9]+',x)
print y

a = 'From: Using the : character'
b = re.findall('^F.+:', a) # greedy
c = re.findall('^F.+?:', a) # NON-greedy
print "greedy:", b
print "non-greedy:", c
```

###2) regex simple examples2
```{r}
import re
x = "From stephen.marquard@uct.ac.za Sat Jan 5 09:14:16 2008"
y = re.findall('\S+@\S+', x) # (>=1) non-whitespace character before @ and then (>=1) non-whitespace character until whitespace 
z = re.findall('^From (\S+@\S+)', x) # same as above but specific extraction for line that starts with From
print y
print z

# extracting only the domain name of the e-mail (aka "uct.ac.za")
a = re.findall('@([^ ]*)',x) # where it has @, extracts after it anything that isn't whitespace >= 0 times until whitespace
b = re.findall('@(\S*)',x)  # same as above basically
print a
print b

# extracting just numbers and collecting into a list (floating num)
hand = open('mbox-short.txt')
numlist = list()
for line in hand:
    line = line.rstrip()
    # ['X-DSPAM-Confidence: 0.8475'] needs to extract #.###
    stuff = re.findall('^X-DSPAM-Confidence: ([0-9.]+)',line) 
    if len(stuff) != 1 : continue
    num = float(stuff[0])
    numlist.append(num)
print "max:", max(numlist)

# extracting special reserved keywords
xxx = 'We just received $10.00 for cookies.'
print re.findall('\$[0-9.]+',xxx)
```

###3) basic connection via socket examples
```{r}
## does not function as instructed due to cloudflare
import socket
mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
mysock.connect(('www.py4inf.com',80)) #socket extablished at this point

mysock.send('GET http://www.py4inf.com/code/romeo.txt HTTP/1.0\n\n')
while True:
    data = mysock.recv(512)
    if(len(data)<1):
        break
    print data;
mysock.close()
```

###4) revisting (3) using urllib
```{r}
## same error due to cloudflare protection system, hence used github raw file format
import urllib
fhand = urllib.urlopen('https://raw.githubusercontent.com/williamdyoon/python-everybody-coursera/master/romeo.txt')
for line in fhand:
    print line.strip()

## urllib allows similar functionality as file handler
counts = dict()
fhand = urllib.urlopen('https://raw.githubusercontent.com/williamdyoon/python-everybody-coursera/master/romeo.txt')
for line in fhand:
    words = line.split()
    for word in words:
        counts[word] = counts.get(word,0) + 1
print counts
```

###5) understanding HTML samples
```{r}
import urllib
fhand = urllib.urlopen('http://www.dr-chuck.com/page1.html')
for line in fhand:
    print line.strip()
```

###6) parsing HTML with BeautifulSoup (REQUIRES BeautifulSoup)
```{r}
import urllib
from BeautifulSoup import *
html = urllib.urlopen('http://www.dr-chuck.com/page1.html').read()
soup = BeautifulSoup(html) # retrieve a list of the anchor tags
tags = soup('a') # each tag is like a dictionary of HTML attributes
for tag in tags:
    print tag.get('href', None) # if not found, set it as None  
```

###6) parsing XML using python basic
```{r}
import xml.etree.ElementTree as ET
data = '''<person>
  <name>Chuck</name>
  <phone type="intl">
    +1 734 303 4456
  </phone>
  <email hide = "yes"/>
</person>'''
tree = ET.fromstring(data)  # simple XML tree formation
print "Name:",tree.find("name").text # can find using tree.find()
print "Attr:",tree.find("email").text
```

###7) parsing XML using python basic2
```{r}
import xml.etree.ElementTree as ET
input = '''<stuff>
  <users>
    <user x="2">
      <id>001</id>
      <name>Chuck</name>
    </user>
    <user x="7">
      <id>009</id>
      <name>Brent</name>
    </user>
  </users>
</stuff>'''
stuff = ET.fromstring(input)
lst = stuff.findall('users/user') # <-- important, find user elements. printing lst is a useless venture
print "->",lst,"<-"
print 'User count:', len(lst)
for item in lst:
  print "name:", item.find("name").text
  print "id:", item.find("id").text # id tag is a text, hence requires only text() after find()
  print "attribute:", item.get("x") # user's x variable is an attribute hence requires get()
``` 

###8) parsing JSON using python basic (dictionary)
```{r}
import json
## json resembles tuples
data = '''{
  "name" : "Chuck",
  "phone" : {
      "type" : "intl",
      "number" : "+1 734 303 4456"
  },
  "email" : {
    "hide" : "yes"
  }
}'''
info = json.loads(data) # loads the string into python format
print type(info)
print "Name:", info["name"]
print "Hide:", info["email"]["hide"]
```

###9) parsing JSON using python basic2 (list)
```{r}
import json
input = '''[
{
  "id":"001",
  "x":"2",
  "name":"Chuck"
},
{
  "id":"009",
  "x":"7",
  "name":"Rawr"
}]'''
info = json.loads(input)
print type(info)
print "User count:", len(info)
for item in info:
  print "Name:", item["name"]
  print "Id:", item["id"]
  print "Attribute:", item["x"]
```

###10) accessing API using python basic
```{r}
## SOAP (Simple Object Access Protocol) (software) = bad, hard to use, overtly complex
  ### remote programs / code which we use over the network
## vs
## REST (REpresentational State Transfer) (resource focused) = better
  ### remote resources which we create, read, update, and delete remotely

import urllib
import json
serviceurl = "http://maps.googleapis.com/maps/api/geocode/json?"

while True:
  address = "Orange County, CA"
  if len(address) < 1 : break
  url = serviceurl + urllib.urlencode({'sensor':'false','address':address})
  print "Retrieving", url
  uh = urllib.urlopen(url)
  data = uh.read()
  print "Retrieved", len(data), "characters"
  
  try: js = json.loads(str(data))
  except: js = None
  
  if 'status' not in js or js['status'] != 'OK':
    print '==== Failure To Retrieve ===='
    print data
    break
  
  #print json.dumps(js, indent=4)
  
  lat = js["results"][0]["geometry"]["location"]["lat"]
  lng = js["results"][0]["geometry"]["location"]["lng"]
  print "lat", lat, "lng", lng
  location = js["results"][0]["formatted_address"]
  print location
  break
```

###10) api security example1
```{r}
## some python 2.7.11 may require installation of oauth as it may not be installed by default
import urllib
from twurl import augment # twurl.py takes in secretive hidden.py that contains 

print '* Calling Twitter...'
url = augment("https://api.twitter.com/1.1/statuses/user_timeline.json", 
              {"screen_name":"CodenameMPIM", "count":"3"})
connection = urllib.urlopen(url)
data = connection.read()
headers = connection.info().dict
print headers
```

###10) api security example2
```{r}
import urllib
import twurl
import json

## warning: usually only 15 requests per 15-minute interval... infinite while True loop broke through the limit initially
TWITTER_URL = "https://api.twitter.com/1.1/statuses/user_timeline.json"
while True:
    acct = "@yameyori"
    if(len(acct)<1): break
    print "Obtaining 5 latest tweets by", acct
    url = twurl.augment(TWITTER_URL, {"screen_name":acct,"count":"5"})
    connection = urllib.urlopen(url)
    data = connection.read()
    headers = connection.info().dict
    print "Remaining", headers["x-rate-limit-remaining"]
    js = json.loads(data)
    for entry in js:
        print entry["created_at"], entry["text"].encode('utf-8')
    break
```