
#Python For Everybody
##3 - Using Python to Access Web Data

###0) python version used

```python
import sys
print(sys.version)
```

```
## 2.7.10 (default, Oct 23 2015, 18:05:06) 
## [GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)]
```

###1) regex simple examples

```python
import re
hand = open('mbox-short.txt')
for line in hand:
    line = line.rstrip()
    if re.search('^From: ', line):
        print line
        break
x = 'My 2 favorite numbers are 19 and 42'
y = re.findall('[0-9]+',x)
print y

a = 'From: Using the : character'
b = re.findall('^F.+:', a) # greedy
c = re.findall('^F.+?:', a) # NON-greedy
print "greedy:", b
print "non-greedy:", c
```

```
## From: stephen.marquard@uct.ac.za
## ['2', '19', '42']
## greedy: ['From: Using the :']
## non-greedy: ['From:']
```

###2) regex simple examples2

```python
import re
x = "From stephen.marquard@uct.ac.za Sat Jan 5 09:14:16 2008"
y = re.findall('\S+@\S+', x) # (>=1) non-whitespace character before @ and then (>=1) non-whitespace character until whitespace 
z = re.findall('^From (\S+@\S+)', x) # same as above but specific extraction for line that starts with From
print y
print z

# extracting only the domain name of the e-mail (aka "uct.ac.za")
a = re.findall('@([^ ]*)',x) # where it has @, extracts after it anything that isn't whitespace >= 0 times until whitespace
b = re.findall('@(\S*)',x)  # same as above basically
print a
print b

# extracting just numbers and collecting into a list (floating num)
hand = open('mbox-short.txt')
numlist = list()
for line in hand:
    line = line.rstrip()
    # ['X-DSPAM-Confidence: 0.8475'] needs to extract #.###
    stuff = re.findall('^X-DSPAM-Confidence: ([0-9.]+)',line) 
    if len(stuff) != 1 : continue
    num = float(stuff[0])
    numlist.append(num)
print "max:", max(numlist)

# extracting special reserved keywords
xxx = 'We just received $10.00 for cookies.'
print re.findall('\$[0-9.]+',xxx)
```

```
## ['stephen.marquard@uct.ac.za']
## ['stephen.marquard@uct.ac.za']
## ['uct.ac.za']
## ['uct.ac.za']
## max: 0.9907
## ['$10.00']
```

###3) basic connection via socket examples

```python
## does not function as instructed due to cloudflare
import socket
mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
mysock.connect(('www.py4inf.com',80)) #socket extablished at this point

mysock.send('GET http://www.py4inf.com/code/romeo.txt HTTP/1.0\n\n')
while True:
    data = mysock.recv(512)
    if(len(data)<1):
        break
    print data;
mysock.close()
```

```
## HTTP/1.1 400 Bad Request
## Date: Fri, 29 Jan 2016 00:36:32 GMT
## Content-Type: text/html
## Connection: close
## Set-Cookie: __cfduid=dd29cda9f64aea1046a8ac2245423a5a21454027792; expires=Sat, 28-Jan-17 00:36:32 GMT; path=/; domain=.py4inf.com; HttpOnly
## Server: cloudflare-nginx
## CF-RAY: 26c0dd0665d613e9-LAX
## 
## <html>
## <head><title>400 Bad Request</title></head>
## <body bgcolor="white">
## <center><h1>400 Bad Request</h1></center>
## <hr><center>cloudflare-nginx</center>
## </body>
## </html>
```

###4) revisting (3) using urllib

```python
## same error due to cloudflare protection system, hence used github raw file format
import urllib
fhand = urllib.urlopen('https://raw.githubusercontent.com/williamdyoon/python-everybody-coursera/master/romeo.txt')
for line in fhand:
    print line.strip()

## urllib allows similar functionality as file handler
counts = dict()
fhand = urllib.urlopen('https://raw.githubusercontent.com/williamdyoon/python-everybody-coursera/master/romeo.txt')
for line in fhand:
    words = line.split()
    for word in words:
        counts[word] = counts.get(word,0) + 1
print counts
```

```
## But soft what light through yonder window breaks
## It is the east and Juliet is the sun
## Arise fair sun and kill the envious moon
## Who is already sick and pale with grief
## {'and': 3, 'envious': 1, 'already': 1, 'fair': 1, 'is': 3, 'through': 1, 'pale': 1, 'yonder': 1, 'what': 1, 'sun': 2, 'Who': 1, 'But': 1, 'moon': 1, 'window': 1, 'sick': 1, 'east': 1, 'breaks': 1, 'grief': 1, 'with': 1, 'light': 1, 'It': 1, 'Arise': 1, 'kill': 1, 'the': 3, 'soft': 1, 'Juliet': 1}
```

###5) understanding HTML samples

```python
import urllib
fhand = urllib.urlopen('http://www.dr-chuck.com/page1.html')
for line in fhand:
    print line.strip()
```

```
## <h1>The First Page</h1>
## <p>
## If you like, you can switch to the
## <a href="http://www.dr-chuck.com/page2.htm">
## Second Page</a>.
## </p>
```

###6) parsing HTML with BeautifulSoup (REQUIRES BeautifulSoup)

```python
import urllib
from BeautifulSoup import *
html = urllib.urlopen('http://www.dr-chuck.com/page1.html').read()
soup = BeautifulSoup(html) # retrieve a list of the anchor tags
tags = soup('a') # each tag is like a dictionary of HTML attributes
for tag in tags:
    print tag.get('href', None) # if not found, set it as None  
```

```
## http://www.dr-chuck.com/page2.htm
```

###6) parsing XML using python basic

```python
import xml.etree.ElementTree as ET
data = '''<person>
  <name>Chuck</name>
  <phone type="intl">
    +1 734 303 4456
  </phone>
  <email hide = "yes"/>
</person>'''
tree = ET.fromstring(data)  # simple XML tree formation
print "Name:",tree.find("name").text # can find using tree.find()
print "Attr:",tree.find("email").text
```

```
## Name: Chuck
## Attr: None
```

###7) parsing XML using python basic2

```python
import xml.etree.ElementTree as ET
input = '''<stuff>
  <users>
    <user x="2">
      <id>001</id>
      <name>Chuck</name>
    </user>
    <user x="7">
      <id>009</id>
      <name>Brent</name>
    </user>
  </users>
</stuff>'''
stuff = ET.fromstring(input)
lst = stuff.findall('users/user') # <-- important, find user elements. printing lst is a useless venture
print "->",lst,"<-"
print 'User count:', len(lst)
for item in lst:
  print "name:", item.find("name").text
  print "id:", item.find("id").text # id tag is a text, hence requires only text() after find()
  print "attribute:", item.get("x") # user's x variable is an attribute hence requires get()
```

```
## -> [<Element 'user' at 0x10e7f1f90>, <Element 'user' at 0x10e7f72d0>] <-
## User count: 2
## name: Chuck
## id: 001
## attribute: 2
## name: Brent
## id: 009
## attribute: 7
```

###8) parsing JSON using python basic (dictionary)

```python
import json
## json resembles tuples
data = '''{
  "name" : "Chuck",
  "phone" : {
      "type" : "intl",
      "number" : "+1 734 303 4456"
  },
  "email" : {
    "hide" : "yes"
  }
}'''
info = json.loads(data) # loads the string into python format
print type(info)
print "Name:", info["name"]
print "Hide:", info["email"]["hide"]
```

```
## <type 'dict'>
## Name: Chuck
## Hide: yes
```

###9) parsing JSON using python basic2 (list)

```python
import json
input = '''[
{
  "id":"001",
  "x":"2",
  "name":"Chuck"
},
{
  "id":"009",
  "x":"7",
  "name":"Rawr"
}]'''
info = json.loads(input)
print type(info)
print "User count:", len(info)
for item in info:
  print "Name:", item["name"]
  print "Id:", item["id"]
  print "Attribute:", item["x"]
```

```
## <type 'list'>
## User count: 2
## Name: Chuck
## Id: 001
## Attribute: 2
## Name: Rawr
## Id: 009
## Attribute: 7
```

###10) accessing API using python basic

```python
## SOAP (Simple Object Access Protocol) (software) = bad, hard to use, overtly complex
  ### remote programs / code which we use over the network
## vs
## REST (REpresentational State Transfer) (resource focused) = better
  ### remote resources which we create, read, update, and delete remotely

import urllib
import json
serviceurl = "http://maps.googleapis.com/maps/api/geocode/json?"

while True:
  address = "Orange County, CA"
  if len(address) < 1 : break
  url = serviceurl + urllib.urlencode({'sensor':'false','address':address})
  print "Retrieving", url
  uh = urllib.urlopen(url)
  data = uh.read()
  print "Retrieved", len(data), "characters"
  
  try: js = json.loads(str(data))
  except: js = None
  
  if 'status' not in js or js['status'] != 'OK':
    print '==== Failure To Retrieve ===='
    print data
    break
  
  #print json.dumps(js, indent=4)
  
  lat = js["results"][0]["geometry"]["location"]["lat"]
  lng = js["results"][0]["geometry"]["location"]["lng"]
  print "lat", lat, "lng", lng
  location = js["results"][0]["formatted_address"]
  print location
  break
```

```
## Retrieving http://maps.googleapis.com/maps/api/geocode/json?sensor=false&address=Orange+County%2C+CA
## Retrieved 1611 characters
## lat 33.7174708 lng -117.8311428
## Orange County, CA, USA
```

###10) api security example1

```python
## some python 2.7.11 may require installation of oauth as it may not be installed by default
import urllib
from twurl import augment # twurl.py takes in secretive hidden.py that contains 

print '* Calling Twitter...'
url = augment("https://api.twitter.com/1.1/statuses/user_timeline.json", 
              {"screen_name":"CodenameMPIM", "count":"3"})
connection = urllib.urlopen(url)
data = connection.read()
headers = connection.info().dict
print headers
```

```
## * Calling Twitter...
## {'content-length': '7934', 'x-rate-limit-reset': '1454121744', 'x-rate-limit-remaining': '179', 'x-xss-protection': '1; mode=block', 'x-content-type-options': 'nosniff', 'x-connection-hash': 'f552a51c3b24542463d4b8212db980ee', 'x-twitter-response-tags': 'BouncerCompliant', 'cache-control': 'no-cache, no-store, must-revalidate, pre-check=0, post-check=0', 'status': '200 OK', 'content-disposition': 'attachment; filename=json.json', 'set-cookie': 'lang=en; Path=/, guest_id=v1%3A145412084433852990; Domain=.twitter.com; Path=/; Expires=Mon, 29-Jan-2018 02:27:24 UTC', 'expires': 'Tue, 31 Mar 1981 05:00:00 GMT', 'x-access-level': 'read-write', 'last-modified': 'Sat, 30 Jan 2016 02:27:24 GMT', 'pragma': 'no-cache', 'date': 'Sat, 30 Jan 2016 02:27:24 GMT', 'x-rate-limit-limit': '180', 'x-response-time': '151', 'x-transaction': '9a9167db393dbe7d', 'strict-transport-security': 'max-age=631138519', 'server': 'tsa_a', 'x-frame-options': 'SAMEORIGIN', 'content-type': 'application/json;charset=utf-8'}
```

###10) api security example2

```python
import urllib
import twurl
import json

## warning: usually only 15 requests per 15-minute interval... infinite while True loop broke through the limit initially
TWITTER_URL = "https://api.twitter.com/1.1/statuses/user_timeline.json"
while True:
    acct = "@yameyori"
    if(len(acct)<1): break
    print "Obtaining 5 latest tweets by", acct
    url = twurl.augment(TWITTER_URL, {"screen_name":acct,"count":"5"})
    connection = urllib.urlopen(url)
    data = connection.read()
    headers = connection.info().dict
    print "Remaining", headers["x-rate-limit-remaining"]
    js = json.loads(data)
    for entry in js:
        print entry["created_at"], entry["text"].encode('utf-8')
    break
```

```
## Obtaining 5 latest tweets by @yameyori
## Remaining 178
## Sat Jan 30 23:46:45 +0000 2016 ë¨¹ê³  ì‹¶ì€ ê²ƒ : ðŸ©ðŸ°ðŸ¦ðŸ®ðŸ˜ðŸ¡ðŸ›ðŸŒ®ðŸ”ðŸ•ðŸŒ­ðŸ—ðŸœðŸ²ðŸðŸ³ðŸžðŸŒ½ðŸ‘ðŸ‰ðŸŒðŸŸðŸ¤ðŸ§€ðŸšðŸºðŸ§ðŸª https://t.co/s7sJ0oPqiB
## Sat Jan 30 22:51:42 +0000 2016 RT @_Nagisa_Shibuya: TOHOã‚·ãƒãƒžã‚ºãªã‚“ã°åº—ã•ã‚“ã§
## ã€Œé“é “å €ã‚ˆã€æ³£ã‹ã›ã¦ãã‚Œï¼Documentary of NMB48ã€ã®èˆžå°æŒ¨æ‹¶ãŠã‚ã‚Šã¾ã—ãŸðŸ˜Š
## 
## æœ€å¾Œã®èˆžå°æŒ¨æ‹¶ã¯ç§é”ã®ãƒ›ãƒ¼ãƒ å¤§é˜ªã§ã•ã›ã¦é ‚ã‘ã¦å¬‰ã—ã‹ã£ãŸã§ã™ðŸ’•
## 
## #é“é “å €ã‚ˆæ³£ã‹ã›ã¦ãã‚Œ
## #å¤§ãƒ’ãƒƒãƒˆæœŸå¾… hâ€¦
## Sat Jan 30 22:46:31 +0000 2016 ë¦¬ë“¬ê²Œìž„ ë°°ê²½ìŒì•…ì´ ê°€ë” í‹°ë¹„ í™ˆì‡¼í•‘ ê´‘ê³ ì˜ ë°°ê²½ìŒì•…ìœ¼ë¡œ ë‚˜ì™€ì„œ í ì¹«í ì¹« ë†€ëžœë‹¤
## Sat Jan 30 22:45:38 +0000 2016 RT @ae3am: ì—¬ìžì–´ë¦°ì´ê°€ 
## ë‚´í”¼ì–´ì‹± ë­ëƒê³  ë¬»ê¸¸ëž˜
## ì–¸ë‹ˆ ì‚¬ì‹¤ ë„ê¹¨ë¹„ì•¼
## ìº¤ë‹¤ê°€ ìš¸ë¦¼...
## ì–´ë¨¸ë‹ˆê°€ ì™œ ìš°ëƒê³  ë¬»ëŠ”ë°
## ë„ë¦¬ë„ë¦¬ ê±°ë¦¬ë©´ì„œ ê·¸ëƒ¥ ìš°ë„¤
## ìˆ¨ê²¨ì£¼ëŠ”ê±°ë‹ˆ
## ë‚´ê°€ ì§„ì§œ ë„ê¹¨ë¹„ì˜€ìœ¼ë©´
## ë„ˆëŠ” ê°ë™ì´ì•¼
## Sat Jan 30 22:45:16 +0000 2016 RT @KaSan_O: ì „ì²  íƒ”ëŠ”ë° ì„œë¡œ ì¼í–‰ìœ¼ë¡œ ë³´ì´ë˜ í• ì•„ë²„ì§€ ë‘ë¶„ì´ ì•‰ì•„ê³„ì‹œë‹¤ê°€ ê°‘ìžê¸° "ì•¼ ìš°ë¦¬ ê¸ˆë°© ë‚´ë¦¬ë‹ˆê¹Œ ì Šì€ ì‚¬ëžŒë“¤ ì•‰ê²Œ ì¼ì–´ë‚˜ìž!ã…‡ã……ã…‡)9"í•˜ê³  ì™¸ì¹˜ì‹¬..ê·¸ëŸ¬ìž ë‹¤ë¥¸ í• ì•„ë²„ì§€ë„ "ê·¸ëž˜!ê·¸ë§ì´ ë§žë‹¤!ã…‡ã……ã…‡)9"í•˜ê³  ë²Œë–¡ ì¼ì–´ë‚¨...
## ì„œâ€¦
```
