
#Python For Everybody
##3 - Using Python to Access Web Data

###0) python version used

```python
import sys
print(sys.version)
```

```
## 2.7.10 (default, Oct 23 2015, 18:05:06) 
## [GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)]
```

###1) regex simple examples

```python
import re
hand = open('mbox-short.txt')
for line in hand:
    line = line.rstrip()
    if re.search('^From: ', line):
        print line
        break
x = 'My 2 favorite numbers are 19 and 42'
y = re.findall('[0-9]+',x)
print y

a = 'From: Using the : character'
b = re.findall('^F.+:', a) # greedy
c = re.findall('^F.+?:', a) # NON-greedy
print "greedy:", b
print "non-greedy:", c
```

```
## From: stephen.marquard@uct.ac.za
## ['2', '19', '42']
## greedy: ['From: Using the :']
## non-greedy: ['From:']
```

###2) regex simple examples2

```python
import re
x = "From stephen.marquard@uct.ac.za Sat Jan 5 09:14:16 2008"
y = re.findall('\S+@\S+', x) # (>=1) non-whitespace character before @ and then (>=1) non-whitespace character until whitespace 
z = re.findall('^From (\S+@\S+)', x) # same as above but specific extraction for line that starts with From
print y
print z

# extracting only the domain name of the e-mail (aka "uct.ac.za")
a = re.findall('@([^ ]*)',x) # where it has @, extracts after it anything that isn't whitespace >= 0 times until whitespace
b = re.findall('@(\S*)',x)  # same as above basically
print a
print b

# extracting just numbers and collecting into a list (floating num)
hand = open('mbox-short.txt')
numlist = list()
for line in hand:
    line = line.rstrip()
    # ['X-DSPAM-Confidence: 0.8475'] needs to extract #.###
    stuff = re.findall('^X-DSPAM-Confidence: ([0-9.]+)',line) 
    if len(stuff) != 1 : continue
    num = float(stuff[0])
    numlist.append(num)
print "max:", max(numlist)

# extracting special reserved keywords
xxx = 'We just received $10.00 for cookies.'
print re.findall('\$[0-9.]+',xxx)
```

```
## ['stephen.marquard@uct.ac.za']
## ['stephen.marquard@uct.ac.za']
## ['uct.ac.za']
## ['uct.ac.za']
## max: 0.9907
## ['$10.00']
```

###3) basic connection via socket examples

```python
## does not function as instructed due to cloudflare
import socket
mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
mysock.connect(('www.py4inf.com',80)) #socket extablished at this point

mysock.send('GET http://www.py4inf.com/code/romeo.txt HTTP/1.0\n\n')
while True:
    data = mysock.recv(512)
    if(len(data)<1):
        break
    print data;
mysock.close()
```

```
## HTTP/1.1 400 Bad Request
## Date: Fri, 29 Jan 2016 00:36:32 GMT
## Content-Type: text/html
## Connection: close
## Set-Cookie: __cfduid=dd29cda9f64aea1046a8ac2245423a5a21454027792; expires=Sat, 28-Jan-17 00:36:32 GMT; path=/; domain=.py4inf.com; HttpOnly
## Server: cloudflare-nginx
## CF-RAY: 26c0dd0665d613e9-LAX
## 
## <html>
## <head><title>400 Bad Request</title></head>
## <body bgcolor="white">
## <center><h1>400 Bad Request</h1></center>
## <hr><center>cloudflare-nginx</center>
## </body>
## </html>
```

###4) revisting (3) using urllib

```python
## same error due to cloudflare protection system, hence used github raw file format
import urllib
fhand = urllib.urlopen('https://raw.githubusercontent.com/williamdyoon/python-everybody-coursera/master/romeo.txt')
for line in fhand:
    print line.strip()

## urllib allows similar functionality as file handler
counts = dict()
fhand = urllib.urlopen('https://raw.githubusercontent.com/williamdyoon/python-everybody-coursera/master/romeo.txt')
for line in fhand:
    words = line.split()
    for word in words:
        counts[word] = counts.get(word,0) + 1
print counts
```

```
## But soft what light through yonder window breaks
## It is the east and Juliet is the sun
## Arise fair sun and kill the envious moon
## Who is already sick and pale with grief
## {'and': 3, 'envious': 1, 'already': 1, 'fair': 1, 'is': 3, 'through': 1, 'pale': 1, 'yonder': 1, 'what': 1, 'sun': 2, 'Who': 1, 'But': 1, 'moon': 1, 'window': 1, 'sick': 1, 'east': 1, 'breaks': 1, 'grief': 1, 'with': 1, 'light': 1, 'It': 1, 'Arise': 1, 'kill': 1, 'the': 3, 'soft': 1, 'Juliet': 1}
```

###5) understanding HTML samples

```python
import urllib
fhand = urllib.urlopen('http://www.dr-chuck.com/page1.html')
for line in fhand:
    print line.strip()
```

```
## <h1>The First Page</h1>
## <p>
## If you like, you can switch to the
## <a href="http://www.dr-chuck.com/page2.htm">
## Second Page</a>.
## </p>
```

###6) parsing HTML with BeautifulSoup (REQUIRES BeautifulSoup)

```python
import urllib
from BeautifulSoup import *
html = urllib.urlopen('http://www.dr-chuck.com/page1.html').read()
soup = BeautifulSoup(html) # retrieve a list of the anchor tags
tags = soup('a') # each tag is like a dictionary of HTML attributes
for tag in tags:
    print tag.get('href', None) # if not found, set it as None  
```

```
## http://www.dr-chuck.com/page2.htm
```

###6) parsing XML using python basic

```python
import xml.etree.ElementTree as ET
data = '''<person>
  <name>Chuck</name>
  <phone type="intl">
    +1 734 303 4456
  </phone>
  <email hide = "yes"/>
</person>'''
tree = ET.fromstring(data)  # simple XML tree formation
print "Name:",tree.find("name").text # can find using tree.find()
print "Attr:",tree.find("email").text
```

```
## Name: Chuck
## Attr: None
```

###7) parsing XML using python basic2

```python
import xml.etree.ElementTree as ET
input = '''<stuff>
  <users>
    <user x="2">
      <id>001</id>
      <name>Chuck</name>
    </user>
    <user x="7">
      <id>009</id>
      <name>Brent</name>
    </user>
  </users>
</stuff>'''
stuff = ET.fromstring(input)
lst = stuff.findall('users/user') # <-- important, find user elements. printing lst is a useless venture
print "->",lst,"<-"
print 'User count:', len(lst)
for item in lst:
  print "name:", item.find("name").text
  print "id:", item.find("id").text # id tag is a text, hence requires only text() after find()
  print "attribute:", item.get("x") # user's x variable is an attribute hence requires get()
```

```
## -> [<Element 'user' at 0x10e7f1f90>, <Element 'user' at 0x10e7f72d0>] <-
## User count: 2
## name: Chuck
## id: 001
## attribute: 2
## name: Brent
## id: 009
## attribute: 7
```

###8) parsing JSON using python basic (dictionary)

```python
import json
## json resembles tuples
data = '''{
  "name" : "Chuck",
  "phone" : {
      "type" : "intl",
      "number" : "+1 734 303 4456"
  },
  "email" : {
    "hide" : "yes"
  }
}'''
info = json.loads(data) # loads the string into python format
print type(info)
print "Name:", info["name"]
print "Hide:", info["email"]["hide"]
```

```
## <type 'dict'>
## Name: Chuck
## Hide: yes
```

###9) parsing JSON using python basic2 (list)

```python
import json
input = '''[
{
  "id":"001",
  "x":"2",
  "name":"Chuck"
},
{
  "id":"009",
  "x":"7",
  "name":"Rawr"
}]'''
info = json.loads(input)
print type(info)
print "User count:", len(info)
for item in info:
  print "Name:", item["name"]
  print "Id:", item["id"]
  print "Attribute:", item["x"]
```

```
## <type 'list'>
## User count: 2
## Name: Chuck
## Id: 001
## Attribute: 2
## Name: Rawr
## Id: 009
## Attribute: 7
```

###10) accessing API using python basic

```python
## SOAP (Simple Object Access Protocol) (software) = bad, hard to use, overtly complex
  ### remote programs / code which we use over the network
## vs
## REST (REpresentational State Transfer) (resource focused) = better
  ### remote resources which we create, read, update, and delete remotely

import urllib
import json
serviceurl = "http://maps.googleapis.com/maps/api/geocode/json?"

while True:
  address = "Orange County, CA"
  if len(address) < 1 : break
  url = serviceurl + urllib.urlencode({'sensor':'false','address':address})
  print "Retrieving", url
  uh = urllib.urlopen(url)
  data = uh.read()
  print "Retrieved", len(data), "characters"
  
  try: js = json.loads(str(data))
  except: js = None
  
  if 'status' not in js or js['status'] != 'OK':
    print '==== Failure To Retrieve ===='
    print data
    break
  
  #print json.dumps(js, indent=4)
  
  lat = js["results"][0]["geometry"]["location"]["lat"]
  lng = js["results"][0]["geometry"]["location"]["lng"]
  print "lat", lat, "lng", lng
  location = js["results"][0]["formatted_address"]
  print location
  break
```

```
## Retrieving http://maps.googleapis.com/maps/api/geocode/json?sensor=false&address=Orange+County%2C+CA
## Retrieved 1611 characters
## lat 33.7174708 lng -117.8311428
## Orange County, CA, USA
```

###10) api security example1

```python
## some python 2.7.11 may require installation of oauth (in my case, yes)
import urllib
from twurl import augment # twurl.py takes in secretive hidden.py that contains 

print '* Calling Twitter...'
url = augment("https://api.twitter.com/1.1/statuses/user_timeline.json", 
              {"screen_name":"CodenameMPIM", "count":"3"})
connection = urllib.urlopen(url)
data = connection.read()
headers = connection.info().dict
print headers
```

```
## * Calling Twitter...
## {'content-length': '7934', 'x-rate-limit-reset': '1454103269', 'x-rate-limit-remaining': '173', 'x-xss-protection': '1; mode=block', 'x-content-type-options': 'nosniff', 'x-connection-hash': '532b81721fc1f6f95aa51d521c97a532', 'x-twitter-response-tags': 'BouncerCompliant', 'cache-control': 'no-cache, no-store, must-revalidate, pre-check=0, post-check=0', 'status': '200 OK', 'content-disposition': 'attachment; filename=json.json', 'set-cookie': 'lang=en; Path=/, guest_id=v1%3A145410308848171614; Domain=.twitter.com; Path=/; Expires=Sun, 28-Jan-2018 21:31:28 UTC', 'expires': 'Tue, 31 Mar 1981 05:00:00 GMT', 'x-access-level': 'read-write', 'last-modified': 'Fri, 29 Jan 2016 21:31:28 GMT', 'pragma': 'no-cache', 'date': 'Fri, 29 Jan 2016 21:31:28 GMT', 'x-rate-limit-limit': '180', 'x-response-time': '40', 'x-transaction': '0701ddcd0ece1ae5', 'strict-transport-security': 'max-age=631138519', 'server': 'tsa_a', 'x-frame-options': 'SAMEORIGIN', 'content-type': 'application/json;charset=utf-8'}
```

###10) api security example2

```python
import urllib
import twurl
import json
#import simplejson as json

## warning: usually only 15 requests per 15-minute interval... infinite while True loop broke through the limit initially
TWITTER_URL = "https://api.twitter.com/1.1/statuses/user_timeline.json"
while True:
    acct = "@yameyori"
    if(len(acct)<1): break
    print "Obtaining 5 latest tweets by", acct, "..."
    url = twurl.augment(TWITTER_URL, {"screen_name":acct,"count":"5"})
    connection = urllib.urlopen(url)
    data = connection.read()
    headers = connection.info().dict
    print "Remaining", headers["x-rate-limit-remaining"]
    js = json.loads(data)
    for entry in js:
        print entry["created_at"], entry["text"].encode('utf-8')
    break
```

```
## Obtaining 5 latest tweets by @yameyori ...
## Remaining 170
## Fri Jan 29 23:54:26 +0000 2016 담벼락 사이에 살고 있는 고양이 두 마리 https://t.co/jrW5teddYt
## Fri Jan 29 23:13:39 +0000 2016 RT @BAMnight_: ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ아 스발 너무 귀여워서 숨을쉴수가 업다ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ https://t.co/tn…
## Fri Jan 29 23:00:24 +0000 2016 RT @ztvaa: ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ(웃다가 죽음) https://t.co/lBQb…
## Fri Jan 29 16:13:39 +0000 2016 먀오가 오랜만에 무대 서서 너무 기분 좋음 ㅠㅠ 먀오야 축하해! https://t.co/2SmYjF3rIs
## Fri Jan 29 14:24:52 +0000 2016 여자도 매력있고 귀여운 여자 좋아하니까 여자가 여돌 판다고 그만 좀 놀라시게...
```
